{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **i. Perkenalan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program ini dapat digunakan untuk mempermudah seller saat memakai marketplace. Program ini dapat menawarkan tipe produk yang cocok digunakan kepada seller saat seller memasukkan produk mereka ke dalam marketplace. Hal ini dilakukan dengan membuat model deep learning klasifikasi berdasarkan data produk dari Januari 2020 hingga Januari 2024. Model tersebut akan memprediksi tipe produk berdasarkan deskripsi produk yang seller tulis. Informasi yang dihasilkan dari model ini nantinya akan dijadikan sebagai bahan pertimbangan untuk seller tentang tipe produk yang cocok digunakan untuk produk mereka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ii. Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A. Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from spacy.cli import download\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **B. Membuat data inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fabric:68% Polyurethane+27% Polyester+5% Cotto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today is supposed to be the happiest day of my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        descriptions\n",
       "0  Fabric:68% Polyurethane+27% Polyester+5% Cotto...\n",
       "1  Today is supposed to be the happiest day of my..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membuat data inference\n",
    "Clothing_product_description = '''Fabric:68% Polyurethane+27% Polyester+5% Cotton,High quality faux Leather\n",
    "Design:Oversized,Plus,Long Sleeve,Faux Leather Jackets,Zip Up Motorcycle Jacket,Fall Outfits,Y2k Fashion,Trendy Clothes,Punk Coat,Moto Biker Outwear,Bomber jacket women.\n",
    "Match:This Jacket is suit for spring, autumn and winter. Just wear a basic T-shirt with jeans for a casual look or wear a dress shirt under it for formal occasions. This all-match style leather jacket must be an indispensable outerwear in your wardrobe.\n",
    "'''\n",
    "\n",
    "book_product_description = '''Today is supposed to be the happiest day of my life.\n",
    "\n",
    "I'm engaged to the man of my dreams, and in a few short hours, I'm going to stand before a judge, who will declare us husband and wife, till death does us part. Despite some bumps in the road, this day is everything I dreamed it would be.\n",
    "\n",
    "There's only one problem:\n",
    "\n",
    "Someone out there doesn't want me to live long enough to say my vows.\n",
    "\n",
    "And if I'm not careful, they may very well get their wish.\n",
    "\n",
    "'''\n",
    "\n",
    "# Mengubah data inference menjadi dataframe\n",
    "data={'descriptions': [Clothing_product_description, book_product_description]}\n",
    "\n",
    "inference_data = pd.DataFrame(data)\n",
    "inference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumber Clothing_product_description : https://www.amazon.com/Leggings-Waisted-Control-See-Through-Workout/dp/B09NJHVJ6W/ref=sr_1_3?crid=22TEZP9CVP5YQ&dib=eyJ2IjoiMSJ9.W8LR5TuvYTbCsmq5NPBKcaQzCj32HXVh8iP5W-nZ__tiuABappvAwl1LCW36ufu9QW9kMG9doOxXe4voJGVdHcW98myLd61se7f2dIHABQteS1-3qgsdWjZwLN-JwJYOQt9WXEw5H867V7ec2UpGaTIPHn8IcZ7sFAeG-GfE5fZblUALl8NwjRAjBjBnNLS8fgflNGZgMTWuMm0z9B75I1y5v5sKHCQQa9RBueJbXJruWgotDAMAy2TB_nJ8-cP0di4UA7E4yscMdgYM_LyzI_UxW06-nSpi8LR_rFdKBDk.uVmEucVP4T4RPPsiMkZLVR_B3J_6GA6slEHnjJXo2VE&dib_tag=se&keywords=clothes&qid=1730823107&sprefix=cloth%2Caps%2C386&sr=8-3\n",
    "\n",
    "Sumber book_product_description : https://www.amazon.com/Housemaids-Wedding-Short-Story/dp/B0DLHLBK74/ref=zg_d_sccl_2/141-2764111-1729026?pd_rd_w=19y6q&content-id=amzn1.sym.7f37c16c-1aa6-48d9-bd2d-34f2cb3ae9e0&pf_rd_p=7f37c16c-1aa6-48d9-bd2d-34f2cb3ae9e0&pf_rd_r=BBV4RXXZZXHJ865QE8TR&pd_rd_wg=nF6D0&pd_rd_r=711abdef-da24-4f20-b076-2d3553c82b13&pd_rd_i=B0DLHLBK74&psc=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C. Pre-processing inference data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghilangkan karakter yang tidak bermakna\n",
    "def f_menghilangkan_karakter_tidak_bermakna(text):\n",
    "  # Mengkecilkan huruf\n",
    "  text = text.lower()\n",
    "\n",
    "  # Menghilangkan karakter spesial dan angka\n",
    "  text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "\n",
    "  # Menghilangkan baris ganda\n",
    "  text = re.sub(r'\\\\n', ' ',text)\n",
    "\n",
    "  # Menghilangkan spasi ganda\n",
    "  text = text.strip()\n",
    "\n",
    "  # Menghilangkan link website\n",
    "  text = re.sub(r\"http\\S+\", \" \", text)\n",
    "  text = re.sub(r\"www.\\S+\", \" \", text)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mnuzu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Mengunduh vocabulary stopwords dari nltk berbahasa inggris\n",
    "nltk.download('stopwords')\n",
    "stpwds_en = list(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghilangkan kata yang tidak bermakna\n",
    "def f_menghilangkan_kata_tidak_bermakna(text):\n",
    "  # Mengubah teks menjadi list berdasarkan spasi\n",
    "  tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "  # Menghilangkan kata stopwords\n",
    "  tokens = [word for word in tokens if word not in stpwds_en]\n",
    "\n",
    "  # Menggabungkan kata pada list menjadi teks\n",
    "  text = ' '.join(tokens)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Mengunduh en_core_web_sm dari spacy\n",
    "download('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk menghilangkan kata yang bermakna sama\n",
    "def f_menghilangkan_kata_bermakna_sama(text):\n",
    "  # Melakukan Lemmanization\n",
    "  tokens = [token.lemma_ for token in nlp(text)]\n",
    "  text = ' '.join(tokens)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    fabric polyurethane polyester cottonhigh quali...\n",
       "1    today suppose happy day life I m engaged man d...\n",
       "Name: descriptions, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghilangkan karakter yang tidak bermakna\n",
    "df_temp = inference_data['descriptions'].apply(lambda x: f_menghilangkan_karakter_tidak_bermakna(x))\n",
    "# Menghilangkan kata yang tidak bermakna dengan stopwords\n",
    "df_temp = df_temp.apply(lambda x: f_menghilangkan_kata_tidak_bermakna(x))\n",
    "# Menghilangkan kata yang bermakna sama dengan lemmanization\n",
    "inference_data_pre_processed = df_temp.apply(lambda x: f_menghilangkan_kata_bermakna_sama(x))\n",
    "inference_data_pre_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **D. Model Prediction with inference data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengambil model yang disimpan\n",
    "loaded_model = tf.keras.models.load_model(\"model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 231ms/step\n",
      "Tipe produk yang terprediksi adalah: ['Clothing & Accessories' 'Books']\n"
     ]
    }
   ],
   "source": [
    "# Memprediksi data inference\n",
    "predictions = loaded_model.predict(inference_data_pre_processed)\n",
    "\n",
    "# Mencari nilai dengan hasil prediksi tertinggi\n",
    "vector_predicted = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Vector mapping\n",
    "mapping_dict = {0: 'Household', 1: 'Books', 2: 'Clothing & Accessories', 3: 'Electronics'}\n",
    "\n",
    "# Mengubah vektor prediksi menjadi tipe produk\n",
    "tipe_produk_predicted = np.vectorize(mapping_dict.get)(vector_predicted)\n",
    "\n",
    "print(\"Tipe produk yang terprediksi adalah:\", tipe_produk_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
